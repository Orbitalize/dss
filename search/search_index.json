{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DSS Deployment","text":""},{"location":"#introduction","title":"Introduction","text":"<p>An operational DSS deployment requires a specific architecture to be compliant with standards requirements and meet performance expectations as described in architecture.  This page describes the deployment procedures recommended by InterUSS to achieve this compliance and meet these expectations.</p>"},{"location":"#deployment-layers","title":"Deployment layers","text":"<p>This repository provides three layers of abstraction to deploy and operate a DSS instance via Kubernetes.</p> <p></p> <p>As described below, InterUSS provides tooling for Kubernetes deployments on Amazon Web Services (EKS) and Google Cloud (GKE). However, you can do this on any supported cloud provider or even on your own infrastructure. Review InterUSS pooling requirements and consult the Kubernetes documentation for your chosen provider.</p> <p>The three layers are the following:</p> <ol> <li>Infrastructure provides instructions and tooling to easily provision a Kubernetes cluster and cloud resources (load balancers, storage...) to a cloud provider. The resulting infrastructure meets the Pooling requirements. Terraform modules are provided for:</li> <li>Amazon Web Services (EKS)</li> <li> <p>Google (GKE)</p> </li> <li> <p>Services provides the tooling to deploy a DSS instance to a Kubernetes cluster.</p> </li> <li>Helm Charts</li> <li> <p>Tanka</p> </li> <li> <p>Operations provides instructions to operate a deployed DSS instance.</p> </li> <li>Pooling procedure</li> <li>Troubleshooting</li> </ol> <p>Depending on your level of expertise and your internal organizational practices, you should be able to use each layer independently or complementary.</p> <p>For local deployment approaches, see the documentation located in the build folder</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>You can find below two guides to deploy a DSS instance from scratch: - Amazon Web Services (EKS) - Google (GKE)</p> <p>For a complete use case, you can look into the configurations of the CI job in operations: ci</p>"},{"location":"#migrations-and-upgrades","title":"Migrations and upgrades","text":"<p>Information related to migrations and upgrades can be found in MIGRATION.md.</p>"},{"location":"#development","title":"Development","text":"<p>The following diagram represents the resources in this repository per layer. </p>"},{"location":"#formatting","title":"Formatting","text":"<p>Terraform files must be formatted using <code>terraform fmt -recursive</code> command to pass the CI linter check.</p>"},{"location":"architecture/","title":"Kubernetes deployment","text":""},{"location":"architecture/#introduction","title":"Introduction","text":"<p>See introduction</p>"},{"location":"architecture/#architecture","title":"Architecture","text":"<p>The expected deployment configuration of a DSS pool supporting a DSS Region is multiple organizations to each host one DSS instance that is interoperable with each other organization's DSS instance.  A DSS pool with three participating organizations (USSs) will have an architecture similar to the diagram below.</p> <p>Note that the diagram shows 2 stateful sets per DSS instance.  Currently, the helm and tanka deployments produce 3 stateful sets per DSS instance.  However, after Issue #481 is resolved, this is expected to be reduced to 2 stateful sets.</p> <p></p>"},{"location":"architecture/#terminology-notes","title":"Terminology notes","text":"<p>See teminology notes.</p>"},{"location":"architecture/#pooling","title":"Pooling","text":""},{"location":"architecture/#objective","title":"Objective","text":"<p>See Pooling Objective and subsections.</p>"},{"location":"architecture/#additional-requirements","title":"Additional requirements","text":"<p>See Additional requirements.</p>"},{"location":"architecture/#survivability","title":"Survivability","text":"<p>One of the primary design considerations of the DSS is to be very resilient to failures.  This resiliency is obtained primarily from the behavior of the underlying CockroachDB database technology and how we configure it.  The diagram below shows the result of failures (bringing a node down for maintenance, or having an entire USS go down) from different starting points, assuming 3 replicas.</p> <p></p> <p>The table below summarizes survivable failures with 3 DSS instances configured according to the architecture described above.  Each system state is summarized by three groups (one group per USS) of two nodes per USS.</p> <ul> <li>\ud83d\udfe9 : Functional node has no recent changes in functionality</li> <li>\ud83d\udfe5 : Non-functional node in down USS has no recent changes in functionality</li> <li>\ud83d\udfe7 : Non-functional node due to USS upgrade or maintenance has no recent changes in functionality</li> <li>\ud83d\udd34 : Node becomes non-functional due to a USS going down</li> <li>\ud83d\udfe0 : Node becomes non-functional due to USS upgrade or maintenance</li> </ul> Pre-existing conditions New failures Survivable? (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe0 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe0 ) \ud83d\udd34 No; some ranges may be lost because of this bug (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe0 ) \ud83d\udd34 No; some ranges may be lost (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udd34 , \ud83d\udd34 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udd34 , \ud83d\udd34 ) (\ud83d\udd34 , \ud83d\udd34 ) \ud83d\udd34 No; ranges guaranteed to be lost (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe7 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe7 ) \ud83d\udd34 No; some ranges may be lost because of this bug (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udd34 , \ud83d\udd34 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udd34 , \ud83d\udd34 ) (\ud83d\udfe9 , \ud83d\udfe7 ) \ud83d\udfe1 Yes, with 3 replicas (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) \ud83d\udfe2 Yes (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udd34 , \ud83d\udd34 ) \ud83d\udfe2 Yes (\ud83d\udd34 , \ud83d\udd34 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) \ud83d\udfe1 Yes, with 3 replicas (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) \ud83d\udfe1 Yes, with 3 replicas (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) \ud83d\udd34 No; ranges guaranteed to be lost (\ud83d\udfe0 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) (\ud83d\udfe0 , \ud83d\udfe7 ) \ud83d\udd34 No; ranges guaranteed to be lost (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udfe9 , \ud83d\udfe7 ) (\ud83d\udd34 , \ud83d\udd34 ) \ud83d\udfe1 Yes, with 3 replicas (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe5 , \ud83d\udfe5 ) (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe5 , \ud83d\udfe5 ) \ud83d\udfe1 Yes, with 3 replicas (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe9 , \ud83d\udfe0 ) (\ud83d\udfe5 , \ud83d\udfe5 ) \ud83d\udd34 No; some ranges may be lost (\ud83d\udfe9 , \ud83d\udfe9 ) (\ud83d\udd34 , \ud83d\udd34 ) (\ud83d\udfe5 , \ud83d\udfe5 ) \ud83d\udd34 No; some ranges may be lost"},{"location":"architecture/#sizing","title":"Sizing","text":""},{"location":"architecture/#introduction_1","title":"Introduction","text":"<p>This section contains an estimate of the computational and other resources likely necessary to support expected demand in a country similar to the United States.</p>"},{"location":"architecture/#time-required-to-fulfill-queries-for-a-single-flight","title":"Time required to fulfill queries for a single flight","text":"<ol> <li>Assume 1 ISA per flight (worst case)<ol> <li>2 ISA management queries per flight (create &amp; delete)</li> </ol> </li> <li>Assume 90% of flights are nominal and require 3 strategic deconfliction queries (Accepted, Activated, Ended) while 10% of flights have problems and require 7 strategic deconfliction queries<ol> <li>3.4 strategic deconfliction queries per flight</li> </ol> </li> <li>Assume 0.1 seconds to fulfill a query<ol> <li>Therefore, 0.54 seconds required (on average) to fulfill management queries to support a flight</li> </ol> </li> </ol>"},{"location":"architecture/#time-required-to-fulfill-queries-for-a-rid-display-provider","title":"Time required to fulfill queries for a RID Display Provider","text":"<ol> <li>Assume 2 Display Providers viewing each flight on average, 4 subscriptions per flight per DP, and 40% chance of subscription reuse<ol> <li>9.6 subscription queries per flight</li> <li>0.96 seconds required (on average) to fulfill viewing queries to support a flight</li> </ol> </li> </ol>"},{"location":"architecture/#required-parallelism","title":"Required parallelism","text":"<ol> <li>Use 348,537 remote pilots in 2024</li> <li>Assume 100 flights per month per remote pilot</li> <li>Use 989,916 recreational pilots as a baseline (even though this is likely number of aircraft, not number of pilots) and double it for the future</li> <li>Use 7.1 flights per month per recreational pilot</li> <li>Therefore, expect about 18.6 flights per second</li> <li>With 1.5 seconds of query time per flight, a nominal parallelism of 28 is required to satisfy the demand</li> <li>Assuming a peak-average ratio of 3.5, a parallelism of 98 is required</li> </ol>"},{"location":"architecture/#required-resources","title":"Required resources","text":"<ol> <li>With Cockroach Labs guidance of 4 parallel operations per vCPU, the DSS pool requires 25 vCPUs.</li> <li>Assuming 3 DSS instances and the need to continue to operate when one instance is down, each DSS instance requires 13 vCPUs.</li> <li>Using 8-vCPU virtual machines (like n2-standard-8), this means each instance needs 2 of these virtual machines</li> <li>Assuming that 5 days' worth of flights are occupying space on disk at any given time and that each flight record on disk is 100k, approximately 83 GB of storage is required<ol> <li>Note that Cockroach Labs recommends 4,000 read IO/s and 4,000 write IO/s, and some cloud providers scale storage speed with storage size, so 83 GB of storage may be far less than is necessary to achieve these speed numbers</li> </ol> </li> </ol>"},{"location":"build/","title":"Deploying a DSS instance","text":""},{"location":"build/#deployment-options","title":"Deployment options","text":"<p>This document describes how to deploy a production-style DSS instance to interoperate with other DSS instances in a DSS pool.</p> <p>To run a local DSS instance for testing, evaluation, or development, see dev/standalone_instance.md.</p> <p>To create a local DSS instance with multi-node CRDB cluster, see dev/mutli_node_local_dss.md.</p> <p>To create or join a pool consisting of multiple interoperable DSS instances, see information on pooling.</p>"},{"location":"build/#glossary","title":"Glossary","text":"<ul> <li>DSS Region - A region in which a single, unified airspace representation is   presented by one or more interoperable DSS instances, each instance typically   operated by a separate organization.  A specific environment (for example,   \"production\" or \"staging\") in a particular DSS Region is called a \"pool\".</li> <li>DSS instance - a single logical replica in a DSS pool.</li> </ul>"},{"location":"build/#preface","title":"Preface","text":"<p>This doc describes a procedure for deploying the DSS and its dependencies (namely CockroachDB) via Kubernetes. The use of Kubernetes is not a requirement, and a DSS instance can join a CRDB cluster constituting a DSS pool as long as it meets the CockroachDB requirements below.</p>"},{"location":"build/#prerequisites","title":"Prerequisites","text":"<p>Download &amp; install the following tools to your workstation:</p> <ul> <li>If deploying on Google Cloud,   install Google Cloud SDK</li> <li>Confirm successful installation with <code>gcloud version</code></li> <li>Run <code>gcloud init</code> to set up a connection to your account.</li> <li><code>kubectl</code> can be installed from <code>gcloud</code> instead of via the method below.</li> <li>Install kubectl to   interact with kubernetes</li> <li>Confirm successful installation with <code>kubectl version --client</code> (should     succeed from any working directory).</li> <li>Note that kubectl can alternatively be installed via the Google Cloud SDK    <code>gcloud</code> shell if using Google Cloud.</li> <li>Install tanka</li> <li>On Linux, after downloading the binary per instructions, run     <code>sudo chmod +x /usr/local/bin/tk</code></li> <li>Confirm successful installation with <code>tk --version</code></li> <li>Install Docker.</li> <li>Confirm successful installation with <code>docker --version</code></li> <li>Install CockroachDB to   generate CockroachDB certificates.</li> <li>These instructions assume CockroachDB Core.</li> <li>You may need to run <code>sudo chmod +x /usr/local/bin/cockroach</code> after     completing the installation instructions.</li> <li>Confirm successful installation with <code>cockroach version</code></li> <li>If developing the DSS codebase,   install Golang</li> <li>Confirm successful installation with <code>go version</code></li> <li>Optionally install Jsonnet if editing   the jsonnet templates.</li> </ul>"},{"location":"build/#docker-images","title":"Docker images","text":"<p>The application logic of the DSS is located in core-service which is provided in a Docker image which is built locally and then pushed to a Docker registry of your choice.  All major cloud providers have a docker registry service, or you can set up your own.</p> <p>To use the prebuilt InterUSS Docker images (without building them yourself), use <code>docker.io/interuss/dss</code> for <code>VAR_DOCKER_IMAGE_NAME</code>.</p> <p>To build these images (and, optionally, push them to a docker registry):</p> <ol> <li> <p>Set the environment variable <code>DOCKER_URL</code> to your docker registry url endpoint.</p> <ul> <li> <p>For Google Cloud, <code>DOCKER_URL</code> should be set similarly to as described     here,     like <code>gcr.io/your-project-id</code> (do not include the image name;     it will be appended by the build script)</p> </li> <li> <p>For Amazon Web Services, <code>DOCKER_URL</code> should be set similarly to as described     here,     like <code>${aws_account_id}.dkr.ecr.${region}.amazonaws.com/</code> (do not include the image name;     it will be appended by the build script)</p> </li> </ul> </li> <li> <p>Ensure you are logged into your docker registry service.</p> <ul> <li> <p>For Google Cloud,     these     are the recommended instructions (<code>gcloud auth configure-docker</code>).     Ensure that     appropriate permissions are enabled.</p> </li> <li> <p>For Amazon Web Services, create a private repository by following the instructions     here, then login     as described here.</p> </li> </ul> </li> <li> <p>Use the <code>build.sh</code> script in this directory to build and push    an image tagged with the current date and git commit hash.</p> </li> <li> <p>Note the VAR_* value printed at the end of the script.</p> </li> </ol>"},{"location":"build/#access-to-private-repository","title":"Access to private repository","text":"<p>See below the description of <code>VAR_DOCKER_IMAGE_PULL_SECRET</code> to configure authentication.</p>"},{"location":"build/#deploying-a-dss-instance-via-kubernetes","title":"Deploying a DSS instance via Kubernetes","text":"<p>This section discusses deploying a Kubernetes service manually, although you can deploy a DSS instance however you like as long as it meets the CockroachDB requirements above. You can do this on any supported cloud provider or even on your own infrastructure. Consult the Kubernetes documentation for your chosen provider.</p> <p>To instead deploy infrastructure using terraform, see the terraform infrastructure deployment page.</p> <p>If you can augment this documentation with specifics for another cloud provider, a PR to that effect would be greatly appreciated.</p> <ol> <li> <p>Create a new Kubernetes cluster. We recommend a new cluster for each DSS     instance.  A reasonable cluster name might be <code>dss-us-prod-e4a</code> (where <code>e4a</code>     is a zone identifier abbreviation), <code>dss-ca-staging</code>,     <code>dss-mx-integration-sae1a</code>, etc.  The name of this cluster will be combined     with other information by Kubernetes to generate a longer cluster context     ID.</p> <ul> <li>On Google Cloud, the recommended procedure to create a cluster is:</li> <li>In Google Cloud Platform, go to the Kubernetes Engine page and under       Clusters click Create cluster.</li> <li>Name the cluster appropriately; e.g., <code>dss-us-prod</code></li> <li>Select Zonal and a compute-zone appropriate to your       geography</li> <li>For the \"default-pool\" node pool:<ul> <li>Enter 3 for number of nodes.</li> <li>In the \"Nodes\" bullet under \"default-pool\", select N2 series and      n2-standard-4 for machine type.</li> </ul> </li> <li>In the \"Networking\" bullet under \"Clusters\", ensure \"Enable VPC       -native traffic\"       is checked.</li> </ul> </li> <li> <p>Make sure correct cluster context is selected by printing the context     name to the console: <code>kubectl config current-context</code></p> <ul> <li> <p>Record this value and use it for <code>$CLUSTER_CONTEXT</code> below; perhaps:    <code>export CLUSTER_CONTEXT=$(kubectl config current-context)</code></p> </li> <li> <p>On Google Cloud, first configure kubectl to interact with the cluster   created above with   these instructions.   Specifically:</p> </li> <li><code>gcloud config set project your-project-id</code></li> <li><code>gcloud config set compute/zone your-compute-zone</code></li> <li><code>gcloud container clusters get-credentials your-cluster-name</code></li> </ul> </li> <li> <p>Ensure the desired namespace is selected; the recommended     namespace is simply <code>default</code> with one cluster per DSS instance.  Print the     the current namespaces with <code>kubectl get namespace</code>.  Use the current     namespace as the value for <code>$NAMESPACE</code> below; perhaps use an environment     variable for convenience: <code>export NAMESPACE=&lt;your namespace&gt;</code>.</p> <p>It may be useful to create a <code>login.sh</code> file with content like that shown below and <code>source login.sh</code> when working with this cluster.</p> <p>GCP: <pre><code>#!/bin/bash\n\nexport CLUSTER_NAME=&lt;your cluster name&gt;\nexport REGION=&lt;GCP region in which your cluster resides&gt;\ngcloud config set project &lt;your GCP project name&gt;\ngcloud config set compute/zone $REGION-a\ngcloud container clusters get-credentials $CLUSTER_NAME\nexport CLUSTER_CONTEXT=$(kubectl config current-context)\nexport NAMESPACE=default\nexport DOCKER_URL=docker.io/interuss\necho \"Current CLUSTER_CONTEXT is $CLUSTER_CONTEXT\n</code></pre></p> </li> <li> <p>Create static IP addresses: one for the Core Service ingress, and one     for each CockroachDB node if you want to be able to interact with other     DSS instances.</p> <ul> <li> <p>If using Google Cloud, the Core Service ingress needs to be created as    a \"Global\" IP address, but the CRDB ingresses as \"Regional\" IP addresses.    IPv4 is recommended as IPv6 has not yet been tested.  Follow    these instructions    to reserve the static IP addresses.  Specifically (replacing    CLUSTER_NAME as appropriate since static IP addresses are defined at    the project level rather than the cluster level), e.g.:</p> <ul> <li><code>gcloud compute addresses create ${CLUSTER_NAME}-backend --global --ip-version IPV4</code></li> <li><code>gcloud compute addresses create ${CLUSTER_NAME}-crdb-0 --region $REGION</code></li> <li><code>gcloud compute addresses create ${CLUSTER_NAME}-crdb-1 --region $REGION</code></li> <li><code>gcloud compute addresses create ${CLUSTER_NAME}-crdb-2 --region $REGION</code></li> </ul> </li> </ul> </li> <li> <p>Link static IP addresses to DNS entries.</p> <ul> <li> <p>Your CockroachDB nodes should have a common hostname suffix; e.g.,    <code>*.db.interuss.com</code>.  Recommended naming is    <code>0.db.yourdeployment.yourdomain.com</code>,    <code>1.db.yourdeployment.yourdomain.com</code>, etc.</p> </li> <li> <p>If using Google Cloud, see    these instructions    to create DNS entries for the static IP addresses created above.  To list    the IP addresses, use <code>gcloud compute addresses list</code>.</p> </li> </ul> </li> <li> <p>(Only if you use CockroachDB) Use <code>make-certs.py</code> script to create certificates for     the CockroachDB nodes in this DSS instance:</p> <pre><code>./make-certs.py --cluster $CLUSTER_CONTEXT --namespace $NAMESPACE\n    [--node-address &lt;ADDRESS&gt; &lt;ADDRESS&gt; &lt;ADDRESS&gt; ...]\n    [--ca-cert-to-join &lt;CA_CERT_FILE&gt;]\n</code></pre> <ol> <li> <p><code>$CLUSTER_CONTEXT</code> is the name of the cluster (see step 2 above).</p> </li> <li> <p><code>$NAMESPACE</code> is the namespace for this DSS instance (see step 3 above).</p> </li> <li> <p><code>Each ADDRESS</code> is the DNS entry for a CockroachDB node that will use the     certificates generated by this command.  This is usually just the nodes     constituting this DSS instance, though if you maintain multiple DSS     instances in a single pool, the separate instances may share     certificates.  Note that <code>--node-address</code> must include all the hostnames     and/or IP addresses that other CockroachDB nodes will use to connect to     your nodes (the nodes using these certificates). Wildcard notation is     supported, so you can use <code>*.&lt;subdomain&gt;.&lt;domain&gt;.com&gt;</code>.  If following     the recommendations above, use a single ADDRESS similar to     <code>*.db.yourdeployment.yourdomain.com</code>.  The ADDRESS entries should be     separated by spaces.</p> </li> <li> <p>If you are pooling with existing DSS instance(s) you need their CA     public cert (ca.crt), which will be concatenated with yours. Set     <code>--ca-cert-to-join</code> to a <code>ca.crt</code> file.  Reach out to existing operators     to request their public cert.  If not joining an existing pool, omit     this argument.</p> </li> <li> <p>Note: If you are creating multiple DSS instances at once, and joining     them together you likely want to copy the nth instance's <code>ca.crt</code> into     the rest of the instances, such that ca.crt is the same across all     instances.</p> </li> </ol> </li> <li> <p>(Only if you use Yugabyte) Use <code>dss-certs.py</code> script to create certificates for the Yugabyte nodes in this DSS instance.</p> </li> <li> <p>If joining an existing DSS pool, share ca.crt with the DSS instance(s) you     are trying to join, and have them apply the new ca.crt, which now contains     both your instance's and the original instance's public certs, to enable     secure bi-directional communication.  Each original DSS instance, upon     receipt of the combined ca.crt from the joining instance, should perform the     actions below.  While they are performing those actions, you may continue     with the instructions.</p> <ol> <li> <p>If you use CockroachDB:</p> <ol> <li>Overwrite its existing ca.crt with the new ca.crt provided by the DSS instance joining the pool.</li> <li>Upload the new ca.crt to its cluster using <code>./apply-certs.sh $CLUSTER_CONTEXT $NAMESPACE</code></li> <li>Restart their CockroachDB pods to recognize the updated ca.crt: <code>kubectl rollout restart statefulset/cockroachdb --namespace $NAMESPACE</code></li> <li>Inform you when their CockroachDB pods have finished restarting (typically around 10 minutes)</li> </ol> </li> <li> <p>If you use Yugabyte</p> <ol> <li>Share your CA with <code>./dss-certs.py get-ca</code></li> <li>Add others CAs of the pool with <code>./dss-certs.py add-pool-ca</code></li> <li>Upload the new CAs to its cluster using <code>./dss-certs.py apply</code></li> <li>Restart their Yugabyte pods to recognize the updated ca.crt: <code>kubectl rollout restart statefulset/yb-master --namespace $NAMESPACE</code> <code>kubectl rollout restart statefulset/yb-tserver --namespace $NAMESPACE</code></li> <li>Inform you when their Yugabyte pods have finished restarting (typically around 10 minutes)</li> </ol> </li> </ol> </li> <li> <p>Ensure the Docker images are built according to the instructions in the     previous section.</p> </li> <li> <p>From this working directory,     <code>cp -r ../deploy/services/tanka/examples/minimum/* workspace/$CLUSTER_CONTEXT</code>.  Note that     the <code>workspace/$CLUSTER_CONTEXT</code> folder should have already been created     by the <code>make-certs.py</code> script.     Replace the imports at the top of <code>main.jsonnet</code> to correctly locate the files:     <pre><code>local dss = import '../../../deploy/services/tanka/dss.libsonnet';\nlocal metadataBase = import '../../../deploy/services/tanka/metadata_base.libsonnet';\n</code></pre></p> </li> <li> <p>If providing a .pem file directly as the public key to validate incoming     access tokens, copy it to dss/build/jwt-public-certs.     Public key specification by JWKS is preferred; if using the JWKS approach     to specify the public key, skip this step.</p> </li> <li> <p>Edit <code>workspace/$CLUSTER_CONTEXT/main.jsonnet</code> and replace all <code>VAR_*</code>     instances with appropriate values:</p> <ol> <li> <p><code>VAR_NAMESPACE</code>: Same <code>$NAMESPACE</code> used in the make-certs.py (and     apply-certs.sh) scripts.</p> </li> <li> <p><code>VAR_CLUSTER_CONTEXT</code>: Same $CLUSTER_CONTEXT used in the <code>make-certs.py</code>     and <code>apply-certs.sh</code> scripts.</p> </li> <li> <p><code>VAR_ENABLE_SCD</code>: Set this boolean true to enable strategic conflict     detection functionality (currently an R&amp;D project tracking an initial     draft of the upcoming ASTM standard).</p> </li> <li> <p><code>VAR_LOCALITY</code>: Unique name for your DSS instance.  Currently, we     recommend \"_\", and the <code>=</code> character is not     allowed.  However, any unique (among all other participating DSS     instances) value is acceptable. <li> <p><code>VAR_DB_HOSTNAME_SUFFIX</code>: The domain name suffix shared by all of your     CockroachDB nodes.  For instance, if your CRDB nodes were addressable at     <code>0.db.example.com</code>, <code>1.db.example.com</code>, and <code>2.db.example.com</code>, then     VAR_DB_HOSTNAME_SUFFIX would be <code>db.example.com</code>.</p> </li> <li> <p><code>VAR_DATASTORE</code>: Datastore to use. Can be set to 'cockroachdb' or 'yugabyte'.</p> </li> <li> <p><code>VAR_CRDB_DOCKER_IMAGE_NAME</code>: Docker image of cockroach db pods. Until     DSS v0.16, the recommended CockroachDB image name is <code>cockroachdb/cockroach:v21.2.7</code>.     From DSS v0.17, the recommended CockroachDB version is <code>cockroachdb/cockroach:v24.1.3</code>.</p> </li> <li> <p><code>VAR_CRDB_NODE_IPn</code>: IP address (numeric) of nth CRDB node (add more     entries if you have more than 3 CRDB nodes).  Example: <code>1.1.1.1</code></p> </li> <li> <p><code>VAR_SHOULD_INIT</code>: Set to <code>false</code> if joining an existing pool, <code>true</code>     if creating the first DSS instance for a pool.  When set <code>true</code>, this     can initialize the data directories on your cluster, and prevent you     from joining an existing pool.</p> </li> <li> <p><code>VAR_EXTERNAL_CRDB_NODEn</code>: Fully-qualified domain name of existing CRDB     nodes if you are joining an existing pool.  If more than three are     available, add additional entries.  If not joining an existing pool,     comment out this <code>JoinExisting:</code> line.</p> <ul> <li>You should supply a minimum of 3 seed nodes to every CockroachDB node.   These 3 nodes should be the same for every node (ie: every node points   to node 0, 1, and 2). For external DSS instances you should point to a   minimum of 3, or you can use a loadbalanced hostname or IP address of   other DSS instances. You should do this for every DSS instance in the   pool, including newly joined instances. See CockroachDB's note on the   join flag.</li> </ul> </li> <li> <p><code>VAR_YUGABYTE_DOCKER_IMAGE_NAME</code>: Docker image of Yugabyte db pods.     Shall be set to at least <code>yugabytedb/yugabyte:2.25.1.0-b381</code></p> </li> <li> <p><code>VAR_YUGABYTE_MASTER_IPn</code>: IP address (numeric) of nth Yugabyte     master node (add more entries if you have more than 3 nodes).     Example: <code>1.1.1.1</code></p> </li> <li> <p><code>VAR_YUGABYTE_TSERVER_IPn</code>: IP address (numeric) of nth Yugabyte     tserver node (add more entries if you have more than 3 nodes).     Example: <code>1.1.1.1</code></p> </li> <li> <p><code>VAR_YUGABYTE_MASTER_ADDRESSn</code>: List of addresses of Yugabyte master     nodes in the DSS pool. Must be accessible from all master/tserver nodes     and identical in a cluster. Example: <code>[\"0.master.db.uss1.example.com\", \"1.master.db.uss1.example.com\", \"3.master.db.uss1.example.com\", \"0.master.db.uss2.example.com\", \"1.master.db.uss2.example.com\", \"3.master.db.uss2.example.com\"]</code>     You may remove this setting if you only have a simple 3-nodes local cluster.</p> </li> <li> <p><code>VAR_YUGABYTE_MASTER_RPC_BIND_ADDRESSES</code>: Bind address for yugabyte     master node. May use <code>${HOSTNAME}</code>, <code>${NAMESPACE}</code> or <code>${HOSTNAMENO}</code>     to use respectively hostname, namespace or number of the node.     Example: <code>${HOSTNAMENO}.master.db.uss1.example.com</code>     You may remove this setting if you only have a simple 3-nodes local cluster.</p> </li> <li> <p><code>VAR_YUGABYTE_MASTER_BROADCAST_ADDRESSES</code>: Broadcast address for yugabyte     master node. May use <code>${HOSTNAME}</code>, <code>${NAMESPACE}</code> or <code>${HOSTNAMENO}</code>     to use respectively hostname, namespace or number of the node.     Example: <code>${HOSTNAMENO}.master.db.uss1.example.com:7100</code>     You may remove this setting if you only have a simple 3-nodes local cluster.</p> </li> <li> <p><code>VAR_YUGABYTE_TSERVER_RPC_BIND_ADDRESSES</code>: Bind address for yugabyte     tserver node. May use <code>${HOSTNAME}</code>, <code>${NAMESPACE}</code> or <code>${HOSTNAMENO}</code>     to use respectively hostname, namespace or number of the node.     Example: <code>${HOSTNAMENO}.tserver.db.uss1.example.com</code>     You may remove this setting if you only have a simple 3-nodes local cluster.</p> </li> <li> <p><code>VAR_YUGABYTE_TSERVER_BROADCAST_ADDRESSES</code>: Broadcast address for yugabyte     tserver node. May use <code>${HOSTNAME}</code>, <code>${NAMESPACE}</code> or <code>${HOSTNAMENO}</code>     to use respectively hostname, namespace or number of the node.     Example: <code>${HOSTNAMENO}.tserver.db.uss1.example.com:9100</code>     You may remove this setting if you only have a simple 3-nodes local cluster.</p> </li> <li> <p><code>VAR_YUGABYTE_FIX_27367_ISSUE</code>: Fix issue 27367     To make the fix working, RPC bind and broadcast addresses must be set to     the same, public value on where the master / tserver node is accessible.</p> </li> <li> <p><code>VAR_YUGABYTE_LIGHT_RESOURCES</code>: Use light resources in term of CPU/Memory     for Yugabyte nodes. You may use that for development purposes, to deploy     a Yugabyte in a small cluster to save costs and resources.</p> </li> <li> <p><code>VAR_YUGABYTE_PLACEMENT_CLOUD</code>: Yugabyte placement's cloud value, for     master and tserver nodes.     Example: <code>cloud-1</code></p> </li> <li> <p><code>VAR_YUGABYTE_PLACEMENT_REGION</code>: Yugabyte placement's region value, for     master and tserver nodes.     Example: <code>uss-1</code></p> </li> <li> <p><code>VAR_YUGABYTE_PLACEMENT_ZONE</code>: Yugabyte placement's zone value, for     master and tserver nodes.     Example: <code>zone-1</code></p> </li> <li> <p><code>VAR_STORAGE_CLASS</code>: Kubernetes Storage Class to use for CockroachDB,     Yugabyte and Prometheus volumes. You can check your cluster's possible     values with <code>kubectl get storageclass</code>. If you're not sure, each cloud     provider has some default storage classes that should work:</p> <ul> <li>Google Cloud: <code>standard</code></li> <li>Azure: <code>default</code></li> <li>AWS: <code>gp2</code></li> </ul> </li> <li> <p><code>VAR_INGRESS_NAME</code>: If using Google Kubernetes Engine, set this to the     the name of the core-service static IP address created above (e.g.,     <code>CLUSTER_NAME-backend</code>).</p> </li> <li> <p><code>VAR_DOCKER_IMAGE_NAME</code>: Full name of the docker image built in the     section above.  <code>build.sh</code> prints this name as the last thing it does     when run with <code>DOCKER_URL</code> set.  It should look something like     <code>gcr.io/your-project-id/dss:2020-07-01-46cae72cf</code> if you built the image     yourself, or <code>docker.io/interuss/dss</code> if using the InterUSS image     without <code>build.sh</code>.</p> <ul> <li>Note that <code>VAR_DOCKER_IMAGE_NAME</code> is used in two places.</li> </ul> </li> <li> <p><code>VAR_DOCKER_IMAGE_PULL_SECRET</code>: Secret name of the credentials to access     the image registry. If the image specified in VAR_DOCKER_IMAGE_NAME does not require     authentication to be pulled, then do not populate this instance and do not uncomment     the line containing it. You can use the following command to store the credentials     as kubernetes secret:</p> <p>kubectl create secret -n VAR_NAMESPACE docker-registry VAR_DOCKER_IMAGE_PULL_SECRET \\     --docker-server=DOCKER_REGISTRY_SERVER \\     --docker-username=DOCKER_USER \\     --docker-password=DOCKER_PASSWORD \\     --docker-email=DOCKER_EMAIL</p> <p>For docker hub private repository, use <code>docker.io</code> as <code>DOCKER_REGISTRY_SERVER</code> and an access token as <code>DOCKER_PASSWORD</code>.</p> </li> <li> <p><code>VAR_APP_HOSTNAME</code>: Fully-qualified domain name of your Core Service     ingress endpoint.  For example, <code>dss.example.com</code>.</p> </li> <li> <p><code>VAR_PUBLIC_ENDPOINT</code>: URL to publicly access your Core Service     ingress endpoint.  For example, <code>https://dss.example.com</code>. Only for versions &gt;=0.21.</p> </li> <li> <p><code>VAR_PUBLIC_KEY_PEM_PATH</code>: If providing a .pem file directly as the     public key to validate incoming access tokens, specify the name of this     .pem file here as <code>/jwt-public-certs/YOUR-KEY-NAME.pem</code> replacing     YOUR-KEY-NAME as appropriate.  For instance, if using the provided     <code>us-demo.pem</code>, use the path     <code>/jwt-public-certs/us-demo.pem</code>.  Note that your .pem file must have     been copied into <code>jwt-public-certs</code> in an earlier     step, or mounted at runtime using a volume.</p> <ul> <li>If providing an access token public key via JWKS, provide a blank   string for this parameter.</li> </ul> </li> <li> <p><code>VAR_JWKS_ENDPOINT</code>: If providing the access token public key via JWKS,     specify the JWKS endpoint here.  Example:     <code>https://auth.example.com/.well-known/jwks.json</code></p> <ul> <li>If providing a .pem file directly as the public key to valid incoming access tokens, provide a blank string for this parameter.</li> </ul> </li> <li> <p><code>VAR_JWKS_KEY_ID</code>: If providing the access token public key via JWKS,     specify the <code>kid</code> (key ID) of they appropriate key in the JWKS file     referenced above.</p> <ul> <li>If providing a .pem file directly as the public key to valid incoming access tokens, provide a blank string for this parameter.</li> </ul> </li> <li> <p>If you are only turning up a single DSS instance for development, you     may optionally change <code>single_cluster</code> to <code>true</code>.</p> </li> <li> <p><code>VAR_SSL_POLICY</code>: When deploying on Google Cloud, a ssl policy     can be applied to the DSS Ingress. This can be used to secure the TLS connection.     Follow the instructions to create the Global SSL Policy and     replace VAR_SSL_POLICY variable with its name. <code>RESTRICTED</code> profile is recommended.     Leave it empty if not applicable.</p> </li> <li> <p><code>VAR_ENABLE_SCHEMA_MANAGER</code>: Set this to true to enable the schema manager jobs.     It is required to perform schema upgrades. Note that it is automatically enabled when <code>VAR_SHOULD_INIT</code> is true.</p> </li> <li> <p>Edit workspace/$CLUSTER_CONTEXT/spec.json and replace all VAR_*     instances with appropriate values:</p> <ol> <li> <p>VAR_API_SERVER: Determine this value with the command:</p> <p><code>echo $(kubectl config view -o jsonpath=\"{.clusters[?(@.name==\\\"$CLUSTER_CONTEXT\\\")].cluster.server}\")</code></p> <ul> <li>Note that <code>$CLUSTER_CONTEXT</code> should be replaced with your actual  <code>CLUSTER_CONTEXT</code> value prior to executing the above command if you  have not defined a <code>CLUSTER_CONTEXT</code> environment variable.</li> </ul> </li> <li> <p>VAR_NAMESPACE: See previous section.</p> </li> </ol> </li> <li> <p>Use the <code>apply-certs.sh</code> script to create secrets on the     Kubernetes cluster containing the certificates and keys generated in the     previous step.</p> <pre><code>./apply-certs.sh $CLUSTER_CONTEXT $NAMESPACE\n</code></pre> </li> <li> <p>Run <code>tk apply workspace/$CLUSTER_CONTEXT</code> to apply it to the     cluster.</p> <ul> <li>If you are joining an existing pool, do not execute this command until the   the existing DSS instances all confirm that their CockroachDB pods have   finished their rolling restarts.</li> </ul> </li> <li> <p>Wait for services to initialize.  Verify that basic services are functioning     by navigating to https://your-domain.example.com/healthy.</p> <ul> <li>On Google Cloud, the highest-latency operation is provisioning of the   HTTPS certificate which generally takes 10-45 minutes.  To track this   progress:</li> <li>Go to the \"Services &amp; Ingress\" left-side tab from the Kubernetes Engine     page.</li> <li>Click on the <code>https-ingress</code> item (filter by just the cluster of     interest if you have multiple clusters in your project).</li> <li>Under the \"Ingress\" section for Details, click on the link corresponding     with \"Load balancer\".</li> <li>Under Frontend for Details, the Certificate column for HTTPS protocol     will have an icon next to it which will change to a green checkmark when     provisioning is complete.</li> <li>Click on the certificate link to see provisioning progress.</li> <li>If everything indicates OK and you still receive a cipher mismatch error     message when attempting to visit /healthy, wait an additional 5 minutes     before attempting to troubleshoot further.</li> </ul> </li> <li> <p>If joining an existing pool, share your CRDB node addresses with the     operators of the existing DSS instances.  They will add these node addresses     to JoinExisting where <code>VAR_CRDB_EXTERNAL_NODEn</code> is indicated in the minimum     example, and then update their deployment:</p> <p><code>tk apply workspace/$CLUSTER_CONTEXT</code></p> </li>"},{"location":"build/#pooling","title":"Pooling","text":"<p>See the pooling documentation.</p>"},{"location":"build/#tools","title":"Tools","text":"<p>See operations monitoring documentation.</p>"},{"location":"build/#troubleshooting","title":"Troubleshooting","text":"<p>See Troubleshooting in <code>deploy/operations</code>.</p>"},{"location":"build/#upgrading-database-schemas","title":"Upgrading Database Schemas","text":"<p>All schemas-related files are in <code>db_schemas</code> directory.  Any changes you wish to make to the database schema should be done in their respective database folders.  The files are applied in sequential numeric steps from the current version M to the desired version N.</p> <p>For the first-ever run during the CRDB cluster initialization, the db-manager will run once to bootstrap and bring the database up to date.  To upgrade existing clusters you will need to:</p>"},{"location":"build/#if-performing-this-operation-on-the-original-cluster","title":"If performing this operation on the original cluster","text":"<ol> <li>Update the <code>desired_xyz_db_version</code> field in <code>main.jsonnet</code></li> <li>Delete the existing db-manager job in your k8s cluster</li> <li>Redeploy the newly configured db-manager with <code>tk apply -t job/&lt;xyz-schema-manager&gt;</code>. It should automatically up/down grade your database schema to your desired version.</li> </ol>"},{"location":"build/#if-performing-this-operation-on-any-other-cluster","title":"If performing this operation on any other cluster","text":"<ol> <li> <p>Create <code>workspace/$CLUSTER_CONTEXT_schema_manager</code> in this (build) directory.</p> </li> <li> <p>From this (build) working directory,     <code>cp -r ../deploy/services/tanka/examples/schema_manager/* workspace/$CLUSTER_CONTEXT_schema_manager</code>.</p> </li> <li> <p>Edit <code>workspace/$CLUSTER_CONTEXT_schema_manager/main.jsonnet</code> and replace all <code>VAR_*</code>     instances with appropriate values where applicable as explained in the above section.</p> </li> <li> <p>Run <code>tk apply workspace/$CLUSTER_CONTEXT_schema_manager</code></p> </li> </ol>"},{"location":"build/#garbage-collector-job","title":"Garbage collector job","text":"<p>Only since commit c789b2b on Aug 25, 2020 will the DSS enable automatic garbage collection of records by tracking which DSS instance is responsible for garbage collection of the record. Expired records added with a DSS deployment running code earlier than this must be manually removed.</p> <p>The Garbage collector job runs every 30 minute to delete records in RID tables that records' endtime is 30 minutes less than current time. If the event takes a long time and takes longer than 30 minutes (previous job is still running), the job will skip a run until the previous job completes.</p>"},{"location":"migration/","title":"CockroachDB and Kubernetes version migration","text":"<p>This page provides information on how to upgrade your CockroachDB and Kubernetes cluster deployed using the tools from this repository.</p>"},{"location":"migration/#cockroachdb-upgrades","title":"CockroachDB upgrades","text":"<p>CockroachDB must be upgraded on all DSS instances of the pool one after the other. The rollout of the upgrades on  the whole CRDB cluster must be carefully performed in sequence to keep the majority of nodes healthy during that period and prevent downtime. For a Pooled deployment, one of the DSS Instance must take the role of the upgrade \"Leader\" and coordinate the upgrade with other \"Followers\" DSS instances. In general a CockroachDB upgrade consists of: 1. Upgrade preparation: Verify that the cluster is in a nominal state ready for upgrade. 1. Decide how the upgrade will be finalized (for major upgrades only): Like CockroachDB, we recommend disabling auto-finalization. 1. Perform the rolling upgrade: This step should be performed first by the Leader and as quickly as possible by the Followers one after the other. Note that during this period, the performance of the cluster may be impacted since, as documented by CockroachDB, \"a query that is sent to an upgraded node can be distributed only among other upgraded nodes. Data accesses that would otherwise be local may become remote, and the performance of these queries can suffer.\" 1. Roll back the upgrade (optional): Like the rolling upgrade, this step should be carefully coordinated with all DSS instances to guarantee the minimum number of healthy nodes to keep the cluster available. 1. Finish the upgrade: This step should be accomplished by the Leader.</p> <p>The following sections provide links to the CockroachDB migration documentation depending on your deployment type, which can be different by DSS instance.</p> <p>Important notes:</p> <ul> <li>Further work is required to test and evaluate the availability of the DSS during migrations.</li> <li>We recommend to review carefully the instructions provided by CockroachDB and to rehearse all migrations on a test   environment before applying them to production.</li> </ul>"},{"location":"migration/#terraform-deployment","title":"Terraform deployment","text":"<p>If a DSS instance has been deployed with terraform, first upgrade the cluster using Helm or Tanka. Then, update the variable <code>crdb_image_tag</code> in your <code>terraform.tfvars</code> to align your configuration with the new state of the cluster.</p>"},{"location":"migration/#helm-deployment","title":"Helm deployment","text":"<p>If you deployed the DSS using the Helm chart and the instructions provided in this repository, follow the instructions provided by CockroachDB <code>Cluster Upgrade with Helm</code> (See specific links below). Note that the CockroachDB documentation suggests to edit the values using <code>helm upgrade ... --set</code> commands. You will need to use the root key <code>cockroachdb</code>  since the cockroachdb Helm chart is a dependency of the dss chart. For instance, setting the image tag and partition using the command line would look like this: <pre><code>helm upgrade [RELEASE_NAME] [PATH_TO_DSS_HELM] --set cockroachdb.image.tag=v24.1.3 --reuse-values\n</code></pre> <pre><code>helm upgrade [RELEASE_NAME] [PATH_TO_DSS_HELM] --set cockroachdb.statefulset.updateStrategy.rollingUpdate.partition=0 --reuse-values\n</code></pre> Alternatively, you can update <code>helm_values.yml</code> in your deployment and set the new image tag and rollout partition like this: <pre><code>cockroachdb:\n  image:\n    # ...\n    tag: # version\n  statefulset:\n    updateStrategy:\n      rollingUpdate:\n        partition: 0\n</code></pre> New values can then be applied using <code>helm upgrade [RELEASE_NAME] [PATH_TO_DSS_HELM] -f [helm_values.yml]</code>. We recommend the second approach to keep your helm values in sync with the cluster state.</p>"},{"location":"migration/#2127-to-2413","title":"21.2.7 to 24.1.3","text":"<p>CockroachDB requires to upgrade one minor version at a time, therefore the following migrations have to be performed:</p> <ol> <li>21.2.7 to 22.1: see CockroachDB Cluster upgrade for Helm.</li> <li>22.1 to 22.2: see CockroachDB Cluster upgrade for Helm.</li> <li>22.2 to 23.1: see CockroachDB Cluster upgrade for Helm.</li> <li>23.1 to 23.2: see CockroachDB Cluster upgrade for Helm.</li> <li>23.2 to 24.1.3: see CockroachDB Cluster upgrade for Helm.</li> </ol>"},{"location":"migration/#tanka-deployment","title":"Tanka deployment","text":"<p>For deployments using Tanka configuration, since no instructions are provided for Tanka specifically, we recommend to follow the manual steps documented by CockroachDB: <code>Cluster Upgrade with Manual configs</code>. (See specific links below) To apply the changes to your cluster, follow the manual steps and reflect the new  values in the Leader and Followers Tanka configurations, namely the new image version (see  <code>VAR_CRDB_DOCKER_IMAGE_NAME</code>) to ensure the new configuration is aligned with the cluster state.</p>"},{"location":"migration/#2127-to-2413_1","title":"21.2.7 to 24.1.3","text":"<p>CockroachDB requires to upgrade one minor version at a time, therefore the following migrations have to be performed:</p> <ol> <li>21.2.7 to 22.1: see CockroachDB Cluster upgrade with Manual configs.</li> <li>22.1 to 22.2: see CockroachDB Cluster upgrade with Manual configs.</li> <li>22.2 to 23.1: see CockroachDB Cluster upgrade with Manual configs.</li> <li>23.1 to 23.2: see CockroachDB Cluster upgrade with Manual configs.</li> <li>23.2 to 24.1.3: see CockroachDB Cluster upgrade with Manual configs.</li> </ol>"},{"location":"migration/#kubernetes-upgrades","title":"Kubernetes upgrades","text":"<p>Important notes:</p> <ul> <li>The migration plan below has been tested with the deployment of services using Helm and Tanka without Istio enabled. Note that this configuration flag has been decommissioned since #995.</li> <li>Further work is required to test and evaluate the availability of the DSS during migrations.</li> <li>It is highly recommended to rehearse such operation on a test cluster before applying them to a production environment.</li> </ul>"},{"location":"migration/#google-google-kubernetes-engine","title":"Google - Google Kubernetes Engine","text":"<p>Migrations of GKE clusters are managed using terraform.</p>"},{"location":"migration/#124-to-132","title":"1.24 to 1.32","text":"<p>For each intermediate version up to the target version (eg. if you upgrade from 1.27 to 1.30, apply thoses  instructions for 1.28, 1.29, 1.30), do:</p> <p>Change your terraform.tfvars to use  by adding or updating the kubernetes_version variable: kubernetes_version =  Run terraform apply. This operation may take more than 30min. Monitor the upgrade of the nodes in the Google Cloud console. <ol> <li>Change your <code>terraform.tfvars</code> to use <code>&lt;new version&gt;</code> by adding or updating the <code>kubernetes_version</code> variable:    <pre><code>kubernetes_version = &lt;new version&gt;\n</code></pre></li> <li>Run <code>terraform apply</code>. This operation may take more than 30min.</li> <li>Monitor the upgrade of the nodes in the Google Cloud console.</li> </ol>"},{"location":"migration/#aws-elastic-kubernetes-service","title":"AWS - Elastic Kubernetes Service","text":"<p>Currently, upgrades of EKS can't be achieved reliably with terraform directly. The recommended workaround is to use the web console of AWS Elastic Kubernetes Service (EKS) to upgrade the cluster. Before proceeding, always check on the cluster page the Upgrade Insights tab which provides a report of the availability of Kubernetes resources in each version. The following sections omit this check if no resource is expected to be reported in the context of a standard deployment performed with the tools in this repository.</p>"},{"location":"migration/#125-to-132","title":"1.25 to 1.32","text":"<ol> <li>Before migrating to 1.29, upgrade aws-load-balancer-controller helm chart on your cluster using <code>terraform apply</code>. Changes introduced by PR #1167. You can verify if the operation has succeeded by running <code>helm list -n kube-system</code>. The APP VERSION shall be <code>2.12</code>.</li> </ol> <p>For each intermediate version up to the target version (eg. if you upgrade from 1.29 to 1.31, apply thoses instructions for 1.29, 1.30, 1.31), do: 1. Upgrade the cluster (control plane) using the AWS console. It should take ~15 minutes. 1. Update the Node Group in the Compute tab with Rolling Update strategy to upgrade the nodes using the AWS console.</p> <p>To finalize the upgrade, change your <code>terraform.tfvars</code> to match the target version (ie 1.32) by adding or updating  the <code>kubernetes_version</code> variable:    <pre><code>kubernetes_version = 1.32\n</code></pre></p>"},{"location":"migration/#124-to-125","title":"1.24 to 1.25","text":"<ol> <li>Check for deprecated resources:<ul> <li>Click on the Upgrade Insights tab to see deprecation warnings on the cluster page.</li> <li>Evaluate errors in Deprecated APIs removed in Kubernetes v1.25. Using <code>kubectl get podsecuritypolicies</code>,   check if there is only one Pod Security Policy named <code>eks.privileged</code>. If it is the case,   according to the AWS documentation, you can proceed.</li> </ul> </li> <li>Upgrade the cluster using the AWS console. It should take ~15 minutes.</li> <li>Change your <code>terraform.tfvars</code> to use <code>1.25</code> by adding or updating the <code>kubernetes_version</code> variable:    <pre><code>kubernetes_version = 1.25\n</code></pre></li> </ol>"},{"location":"infrastructure/","title":"Infrastructure","text":"<p>As a phase in DSS deployment, this folder contains the terraform modules required to prepare the infrastructure to host a DSS deployment.  To deploy infrastructure manually (rather than terraform, as described here), see \"Deploying a DSS instance via Kubernetes\".</p> <p>See Services to deploy the DSS once the infrastructure is ready.</p>"},{"location":"infrastructure/#modules","title":"Modules","text":"<p>The modules directory contains the terraform public modules required to prepare the infrastructure on various cloud providers.</p> <ul> <li>terraform-aws-dss: Amazon Web Services deployment</li> <li>terraform-google-dss: Google Cloud Engine deployment</li> </ul> <p>This terraform module creates a Kubernetes cluster in Amazon Web Services using the Elastic Kubernetes Service (EKS) and generates the tanka files to deploy a DSS instance. This terraform module creates a Kubernetes cluster in Google Cloud Engine and generates the tanka files to deploy a DSS instance.</p>"},{"location":"infrastructure/#dependencies","title":"Dependencies","text":"<p>The dependencies directory contains submodules used by the public modules described above. They are not expected to be used directly by users. Those submodules are the combination of the cloud specific dependencies <code>terraform-*-kubernetes</code> and <code>terraform-common-dss</code>. <code>terraform-common-dss</code> module aggregates and outputs the infrastructure configuration which can be used as input to the <code>Services</code> deployment as shown in the diagram below.</p> <p></p>"},{"location":"infrastructure/#local","title":"Local","text":"<p>The local directory contains various documentation that can be used to spawn a cluster locally.</p> <ul> <li>minikuke: Minikube local deployment</li> </ul>"},{"location":"infrastructure/#utils","title":"Utils","text":"<p>This utils folder contains scripts to help manage the terraform modules and dependencies. See the README in that folder for details.</p>"},{"location":"infrastructure/cloud/cleanup/","title":"Clean up","text":"<p>Note that the following operations can't be reverted and all data will be lost.</p>"},{"location":"infrastructure/cloud/cleanup/#aws","title":"AWS","text":"<ol> <li>To delete all resources, run <code>tk delete .</code> in the workspace folder.</li> <li>Make sure that all load balancers and target groups have been deleted from the AWS region before next step.</li> <li><code>terraform destroy</code> in your infrastructure folder.</li> <li>On the EBS page, make sure to manually clean up the persistent storage. Note that the correct AWS region shall be selected.</li> </ol>"},{"location":"infrastructure/cloud/cleanup/#gcp","title":"GCP","text":"<p>To delete all resources, run <code>terraform destroy</code>. Note that this operation can't be reverted and all data will be lost.</p> <p>For Google Cloud Engine, make sure to manually clean up the persistent storage: https://console.cloud.google.com/compute/disks</p>"},{"location":"infrastructure/cloud/cluster/","title":"Deploy Kubernetes cluster","text":"<ol> <li>Create a new folder in <code>/deploy/infrastructure/personal/</code> named for instance <code>terraform-cloud-dss-dev</code>.</li> <li>Copy main.tf, output.tf and variables.gen.tf to the new folder.</li> <li>Copy <code>terraform.dev.example.tfvars</code> and rename to <code>terraform.tfvars</code></li> <li>Check that your new directory contains the following files:<ul> <li>main.tf</li> <li>output.tf</li> <li>terraform.tfvars</li> <li>variables.gen.tf</li> </ul> </li> <li>Set the variables in <code>terraform.tfvars</code> according to your environment. See TFVARS.gen.md for variables descriptions.    TODO: differnt depending on provider</li> <li>In the new directory (i.e. /deploy/infrastructure/personal/terraform-cloud-dss-dev), initialize terraform: <code>terraform init</code>.</li> <li>Run <code>terraform plan</code> to check that the configuration is valid. It will display the resources which will be provisioned.</li> <li>Run <code>terraform apply</code> to deploy the cluster. (This operation may take up to 15 min.)</li> </ol>"},{"location":"infrastructure/cloud/dns/","title":"Setup DNS","text":"<p>This page describes the options and steps required to setup DNS for a DSS deployment.</p>"},{"location":"infrastructure/cloud/dns/#terraform-managed","title":"Terraform managed","text":"<p>If your DNS zone is managed on the same account, it is possible to instruct terraform to create and manage it with the rest of the infrastructure.</p> AWSGCP <p>For Elastic Kubernetes Service (AWS), create the zone in your aws account and set the <code>aws_route53_zone_id</code> variable with the zone id. Entries will be automatically created by terraform.  Note that the domain or the sub-domain managed by the zone must be properly delegated by the parent domain. See instructions for subdomains delegation</p> <p>For Google Cloud Engine, configure the zone in your google account and set the <code>google_dns_managed_zone_name</code> variable the zone name. Zones can be listed by running <code>gcloud dns managed-zones list</code>. Entries will be automatically created by terraform.</p>"},{"location":"infrastructure/cloud/dns/#manual-setup","title":"Manual setup","text":"<p>If DNS entries are managed manually, set them up manually using the following steps:</p>"},{"location":"infrastructure/cloud/dns/#aws","title":"AWS","text":"<ol> <li>Retrieve IP addresses and expected hostnames: <code>terraform output</code>    Example of expected output:    <pre><code>crdb_addresses = [\n    {\n        \"address\" = \"34.65.15.23\"\n        \"expected_dns\" = \"0.interuss.example.com\"\n    },\n    {\n        \"address\" = \"34.65.146.56\"\n        \"expected_dns\" = \"1.interuss.example.com\"\n    },\n    {\n        \"address\" = \"34.65.191.145\"\n        \"expected_dns\" = \"2.interuss.example.com\"\n    },\n]\ngateway_address = {\n    \"address\" = \"35.186.236.146\"\n    \"expected_dns\" = \"dss.interuss.example.com\"  \n    \"certificate_validation_dns\" = [\n     {\n       \"managed_by_terraform\" = false\n       \"name\" = \"_6e246283563dcf58e7ed.interuss.example.com.\"\n       \"records\" = [\n          \"_6e246283563dcf58e7ed.xxxxx.acm-validations.aws.\",\n       ]\n       \"type\" = \"CNAME\"\n     },\n    ]\n}\n</code></pre></li> <li> <p>Create the following DNS A entries to point to the static ips:</p> <ul> <li><code>crdb_addresses[*].expected_dns</code></li> <li><code>gateway_address.expected_dns</code></li> </ul> </li> <li> <p>Create the entries for SSL certificate validation according to the information provided     in <code>gateway_address.certificate_validation_dns</code>.</p> </li> </ol>"},{"location":"infrastructure/cloud/dns/#gcp","title":"GCP","text":"<ol> <li>Retrieve IP addresses and expected hostnames: <code>terraform output</code>    Example of expected output:    <code>crdb_addresses = [        {            \"address\" = \"34.65.15.23\"            \"expected_dns\" = \"0.interuss.example.com\"        },        {            \"address\" = \"34.65.146.56\"            \"expected_dns\" = \"1.interuss.example.com\"        },        {            \"address\" = \"34.65.191.145\"            \"expected_dns\" = \"2.interuss.example.com\"        },    ]    gateway_address = {        \"address\" = \"35.186.236.146\"        \"expected_dns\" = \"dss.interuss.example.com\"    }</code></li> <li>Create the related DNS A entries to point to the static ips.</li> </ol>"},{"location":"infrastructure/cloud/dss/","title":"Deploy DSS services","text":"<p>During the successful run, the terraform job has created a new workspace for the cluster. The new workspace name corresponds to the cluster context. The cluster context can be retrieved by running <code>terraform output</code> in your infrastructure  folder (ie /deploy/infrastructure/personal/terraform-aws-dss-dev).</p> <p>It contains scripts to operate the cluster and setup the services.</p> <ol> <li>Go to the new workspace <code>/build/workspace/${cluster_context}</code>.</li> <li>Run <code>./get-credentials.sh</code> to login to kubernetes. You can now access the cluster with <code>kubectl</code>.</li> <li>If using CockroachDB:<ol> <li>Generate the certificates using <code>./make-certs.sh</code>. Follow script instructions if you are not initializing the cluster.</li> <li>Deploy the certificates using <code>./apply-certs.sh</code>.</li> </ol> </li> <li>If using Yugabyte:<ol> <li>Generate the certificates using <code>./dss-certs.sh init</code></li> <li>If joining a cluster, check <code>dss-certs.sh</code>'s help to add others CA in your pool and share your CA with others pools members.</li> <li>Deploy the certificates using <code>./dss-certs.sh apply</code>.</li> </ol> </li> <li>Run <code>tk apply .</code> to deploy the services to kubernetes. (This may take up to 30 min)</li> <li>Wait for services to initialize:<ul> <li>On AWS, load balancers and certificates are created by Kubernetes Operators. Therefore, it may take few minutes (~5min) to get the services up and running and generate the certificate. To track this progress, go to the following pages and check that:<ul> <li>On the EKS page, the status of the kubernetes cluster should be <code>Active</code>.</li> <li>On the EC2 page, the load balancers (1 for the gateway, 1 per cockroach nodes) are in the state <code>Active</code>.</li> </ul> </li> <li>On Google Cloud, the highest-latency operation is provisioning of the HTTPS certificate which generally takes 10-45 minutes. To track this progress:<ul> <li>Go to the \"Services &amp; Ingress\" left-side tab from the Kubernetes Engine page.</li> <li>Click on the https-ingress item (filter by just the cluster of interest if you have multiple clusters in your project).</li> <li>Under the \"Ingress\" section for Details, click on the link corresponding with \"Load balancer\".</li> <li>Under Frontend for Details, the Certificate column for HTTPS protocol will have an icon next to it which will change to a green checkmark when provisioning is complete.</li> <li>Click on the certificate link to see provisioning progress.</li> <li>If everything indicates OK and you still receive a cipher mismatch error message when attempting to visit /healthy, wait an additional 5 minutes before attempting to troubleshoot further.</li> </ul> </li> </ul> </li> <li>Verify that basic services are functioning by navigating to https://your-gateway-domain.com/healthy.</li> </ol>"},{"location":"infrastructure/cloud/prereqs/","title":"Prerequisites","text":""},{"location":"infrastructure/cloud/prereqs/#terraform","title":"Terraform","text":"<p>Install Terraform.</p>"},{"location":"infrastructure/cloud/prereqs/#kubernetes-cluster-configuration","title":"Kubernetes cluster configuration","text":"TankaHelm <ul> <li>Install tanka</li> <li>On Linux, after downloading the binary per instructions, run <code>sudo chmod +x /usr/local/bin/tk</code></li> <li>Confirm successful installation with <code>tk --version</code></li> <li>Optionally install Jsonnet if editing the jsonnet templates.</li> </ul> <p>TBD</p>"},{"location":"infrastructure/cloud/prereqs/#database-configuration","title":"Database configuration","text":"CockroachDBYugabyte <p>TBD</p> <p>Install CockroachDB to generate CockroachDB certificates.</p> <ul> <li>These instructions assume CockroachDB Core.</li> <li>You may need to run <code>sudo chmod +x /usr/local/bin/cockroach</code> after completing the installation instructions.</li> <li>Confirm successful installation with <code>cockroach version</code></li> </ul>"},{"location":"infrastructure/cloud/prereqs/#cloud-cli-client","title":"Cloud CLI client","text":"AWSGCP <ol> <li>Install and initialize AWS CLI.</li> <li>Confirm successful installation with <code>aws --version</code>.</li> <li>If you don't have an account, sign-up: https://aws.amazon.com/free/</li> <li>Configure terraform to connect to AWS using your account.</li> <li>We recommend to create an AWS_PROFILE using for instance <code>aws configure --profile aws-interuss-dss</code> Before running <code>terraform</code> commands, run once in your shell: <code>export AWS_PROFILE=aws-interuss-dss</code> Other methods are described here: https://registry.terraform.io/providers/hashicorp/aws/latest/docs#authentication-and-configuration</li> </ol> <ol> <li>Install and initialize Google Cloud CLI.</li> <li>Confirm successful installation with <code>gcloud version</code>.</li> <li>Check that the DSS project is correctly selected: gcloud config list project</li> <li>Set another one if needed using: <code>gcloud config set project $GOOGLE_PROJECT_NAME</code></li> <li>Enable the following API using Google Cloud CLI:</li> <li><code>compute.googleapis.com</code></li> <li><code>container.googleapis.com</code></li> <li>If you want to manage DNS entries with terraform: <code>dns.googleapis.com</code></li> <li>Install the auth plugin to connect to kubernetes: <code>gcloud components install gke-gcloud-auth-plugin</code></li> <li>Run <code>gcloud auth application-default login</code> to generate credentials to call Google Cloud Platform APIs.</li> <li>If the result of performing the authorization indicates 404 in the browser, check whether a local dummy-oauth instance is running (using port 8085).  Stop the dummy-oauth instance if it is running.</li> </ol>"},{"location":"infrastructure/dependencies/terraform-aws-kubernetes/","title":"terraform-aws-kubernetes","text":"<p>To deploy a complete DSS to AWS Elastic Kubernetes Service, see terraform-aws-dss.</p> <p>This folder only contains the terraform module which deploys the kubernetes cluster required to run the DSS on Kubernetes in AWS.</p>"},{"location":"infrastructure/dependencies/terraform-aws-kubernetes/#configuration","title":"Configuration","text":"<p>See variables.gen.tf.</p>"},{"location":"infrastructure/dependencies/terraform-aws-kubernetes/#design","title":"Design","text":"<p>This module creates an EKS cluster with related worker nodes. EKS requires 2 subnets in different availability zones ( AZ). A dedicated VPC is created to that effect. At the moment, worker nodes are deployed in the two first AZ of the region. The following table summarizes current responsibilities for resource creation in the AWS account:</p> Resource type Manager VPC and Subnets Terraform Elastic IPs Terraform Network Load balancer Kubernetes controller: aws-load-balancer-controller Target groups Kubernetes controller: aws-load-balancer-controller Storage Volumes (Elastic Block Storage) EKS add-on provisioned by terraform SSL Certificates (AWS Certificates Manager) Terraform DNS Terraform (or manual)"},{"location":"infrastructure/dependencies/terraform-aws-kubernetes/#aws-load-balancers-and-kubernetes-services","title":"AWS Load Balancers and Kubernetes Services","text":"<p>Load balancers are provisioned by the Kubernetes controller aws-load-balancer-controller v2.12 with Option A for IAM configuration.</p> <p>Network Load Balancers map elastic IPs to Kubernetes Services (Load Balancer). Application Load Balancers (Ingress) do not support this feature at the moment, making impossible to anticipate DNS records inside the cluster.</p> <p>The Network Load Balancers are provisioned by the aws-load-balancer-controller. It handles the TLS termination for the dss https service.</p> <p>Note that the load balancer is distributing the traffic to possibly multiple subnets. Be aware that it is not possible to unassign a subnet. Target pods shall be deployed in every subnet, meaning that the pods should be properly distributed in worker nodes and a worker node should be at least present in each subnets.</p> <p>Provisioning is done by annotating a Kubernetes Service resource. The following example deploys a simple http server:</p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: example\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-app\n  namespace: example\n  labels:\n    app: example-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: example-app\n  template:\n    metadata:\n      labels:\n        app: example-app\n    spec:\n      containers:\n        - name: nginx\n          image: public.ecr.aws/nginx/nginx:1.21\n          ports:\n            - name: http\n              containerPort: 80\n          imagePullPolicy: IfNotPresent\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: example\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: external\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip\n    service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\n    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: [ CERTIFICATE_ARN ]\n    service.beta.kubernetes.io/aws-load-balancer-eip-allocations: [ EIP_ALLOCATION_ID1,EIP_ALLOCATION_ID2,... ]\n    service.beta.kubernetes.io/aws-load-balancer-name: [ LOAD_BALANCER_NAME ]\n    service.beta.kubernetes.io/aws-load-balancer-subnets: [ SUBNET_ID1,SUBNET_ID2,... ]\nspec:\n  selector:\n    app: example-app\n  ports:\n    - port: 443\n      targetPort: 80\n      protocol: TCP\n      name: http\n  type: LoadBalancer\n  loadBalancerClass: service.k8s.aws/nlb\n</code></pre> <ul> <li>[CERTIFICATE_ARN]: arn of the certificate managed by AWS Certificate Manager</li> <li>[EIP_ALLOCATION_IDx]: Elastic IP allocation id (The number of elastic IP should equal to the number of SUBNET_IDx)</li> <li>[LOAD_BALANCER_NAME]: Name of the balancer created by the controller</li> <li>[SUBNET_IDx]: Name or ID of a subnet (The number of subnets should equal to the number of EIP_ALLOCATION_IDx)</li> </ul>"},{"location":"infrastructure/dependencies/terraform-aws-kubernetes/#test","title":"Test","text":"<p><code>terraform apply</code> generates a resource specification <code>test-app.yml</code>. This file can be applied to test a http server reachability within the deployed cluster. To apply the resources, follow the next steps:</p> <ol> <li>Login to the EKS cluster: <code>aws eks --region $AWS_REGION update-kubeconfig --name $CLUSTER_NAME</code></li> <li>Create the resources: <code>kubectl apply -f test-app.yml</code></li> <li>Wait (up to 5min) for the load balancer to be ready. Note that the load balancer may take few minutes to start.    Monitor the progress here until the state becomes <code>Active</code>: https://console.aws.amazon.com/ec2/home#LoadBalancers:</li> <li>Connect to the app_hostname and contemplate the nginx default welcome page.</li> </ol>"},{"location":"infrastructure/dependencies/terraform-aws-kubernetes/#clean-up-test","title":"Clean up test","text":"<p>Delete the resources: <code>kubectl delete -f test-app.yml</code>.</p>"},{"location":"infrastructure/dependencies/terraform-aws-kubernetes/#clean-up-infrastructure","title":"Clean up infrastructure","text":"<ol> <li>Delete all created resources from the cluster (eg. clean up test as described in the previous section.)</li> <li>Make sure all load balancers and target groups have been removed.</li> <li>Run <code>terraform destroy</code>.</li> </ol>"},{"location":"infrastructure/dependencies/terraform-commons-dss/","title":"terraform-commons-dss","text":"<p>This folder contains a terraform module which gathers resources required by all cloud providers.</p> <p>It currently consists of the automatic generation of the tanka configuration to deploy  the Kubernetes resources as well as the scripts required to generate the certificates  and operate the cluster.</p>"},{"location":"infrastructure/dependencies/terraform-commons-dss/#configuration","title":"Configuration","text":"<p>See variables.gen.tf.</p>"},{"location":"infrastructure/dependencies/terraform-google-kubernetes/","title":"terraform-google-kubernetes","text":"<p>To deploy a complete DSS to Google Cloud Engine, see terraform-google-dss.</p> <p>This folder only contains the terraform module which deploys the kubernetes cluster required to run the DSS on Kubernetes in Google Cloud Engine.</p>"},{"location":"infrastructure/dependencies/terraform-google-kubernetes/#configuration","title":"Configuration","text":"<p>See variables.gen.tf.</p>"},{"location":"infrastructure/local/minikube/","title":"minikube","text":"<p>This module provide instructions to prepare a local minikube cluster.</p> <p>Minikube is going to take care of most of the work by spawning a local kubernetes cluster.</p>"},{"location":"infrastructure/local/minikube/#getting-started","title":"Getting started","text":""},{"location":"infrastructure/local/minikube/#prerequisites","title":"Prerequisites","text":"<p>Download &amp; install the following tools to your workstation:</p> <ol> <li>Install minikube (First step only).</li> <li>Install tools from Prerequisites</li> </ol>"},{"location":"infrastructure/local/minikube/#create-a-new-minikube-cluster","title":"Create a new minikube cluster","text":"<ol> <li>Run <code>minikube start -p dss-local-cluster</code> to create a new cluster.</li> <li>Run <code>minikube tunnel -p dss-local-cluster</code> and keep it running to expose LoadBalancer services.</li> </ol> <p>If needed, you can change the name of the cluster (<code>dss-local-cluster</code> in this documentation) as needed. You may also deploy multiple cluster at the same time, using different names.</p>"},{"location":"infrastructure/local/minikube/#access-to-the-cluster","title":"Access to the cluster","text":"<p>Minikube provide a UI, should you want to keep track of deployment and/or inspect the cluster. To start it, use the following command:</p> <ol> <li><code>minikube dashboard -p dss-local-cluster</code></li> </ol> <p>You can also use any other tool as needed. You can switch to the cluster's context by using the following command:</p> <ol> <li><code>kubectl config use-context dss-local-cluster</code></li> </ol>"},{"location":"infrastructure/local/minikube/#upload-or-update-local-image","title":"Upload or update local image","text":"<p>Should you want to run the local docker image that you built, run the following commands to upload / update your image</p> <ol> <li><code>minikube image -p dss-local-cluster push interuss-local/dss</code></li> </ol> <p>In the helm charts, use <code>docker.io/interuss-local/dss:latest</code> as image and be sure to set the <code>imagePullPolicy</code> to <code>Never</code>.</p>"},{"location":"infrastructure/local/minikube/#deployment-of-the-dss-services","title":"Deployment of the DSS services","text":"<p>You can now deploy the DSS services using helm charts.</p> <p>Follow the instructions in the README, especially the ones related to certificate generation and publication to the cluster. However, there are some minikube specific things to do:</p> <ul> <li>Use the <code>global.cloudProvider</code> setting with the value <code>minikube</code> and deploy the charts on the <code>dss-local-cluster</code> kubernetes context.</li> <li>To access the service, find the external IP using the <code>kubectl get services dss-dss-gateway</code> command. The port 80, without HTTPs is used.</li> </ul> <p>You may also use the tanka files to deploy the service. An example configuration is provided there.</p>"},{"location":"infrastructure/local/minikube/#clean-up","title":"Clean up","text":"<p>To delete all resources, run <code>minikube delete -p dss-local-cluster</code>.  Note that this operation can't be reverted and all data will be lost.</p>"},{"location":"infrastructure/local/prereqs/","title":"Prerequisites","text":"<p>Download &amp; install the following tools to your Linux workstation:</p> <ul> <li> <p>Install kubectl to   interact with kubernetes</p> <ul> <li>Confirm successful installation with <code>kubectl version --client</code> (should   succeed from any working directory).</li> </ul> </li> <li> <p>Install Docker.</p> <ul> <li>Confirm successful installation with <code>docker --version</code></li> </ul> </li> <li> <p>If developing the DSS codebase,   install Golang</p> <ul> <li>Confirm successful installation with <code>go version</code></li> </ul> </li> <li> <p>Install minikube (First step only).</p> </li> </ul>"},{"location":"infrastructure/local/prereqs/#configuration","title":"Configuration","text":"HelmTanka <p>TBD</p> <ul> <li>Install tanka<ul> <li>On Linux, after downloading the binary per instructions, run   <code>sudo chmod +x /usr/local/bin/tk</code></li> <li>Confirm successful installation with <code>tk --version</code></li> </ul> </li> <li>Optionally install Jsonnet if editing the jsonnet templates.</li> </ul>"},{"location":"infrastructure/local/prereqs/#database","title":"Database","text":"CockroachDBYugabyte <p>TBD</p> <p>Install CockroachDB to generate CockroachDB certificates.</p> <ul> <li>These instructions assume CockroachDB Core.</li> <li>You may need to run <code>sudo chmod +x /usr/local/bin/cockroach</code> after completing the installation instructions.</li> <li>Confirm successful installation with <code>cockroach version</code></li> </ul>"}]}